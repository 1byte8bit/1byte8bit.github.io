[{"title":"Goodbye Microservices:From 100s of problem children to 1 superstar","url":"/2019/05/25/c827ce44-307a-4240-aa90-60fd384e31f4/","content":"本文转自：[https://segment.com/blog/goodbye-microservices/](https://segment.com/blog/goodbye-microservices/)\n这就是那个最值钱的问题：\nHow do you tell the difference between:\n1: XXX is a wrong solution.\n2: XXX is not a bad solution, but you are doing it wrong.\n以下是原文：\n<!--more-->\nUnless you’ve been living under a rock, you probably already know that microservices is the architecture du jour. Coming of age alongside this trend, Segment[ adopted this as a best practice ](https://segment.com/blog/why-microservices/)early-on, which served us well in some cases, and, as you’ll soon learn, not so well in others.\n\nBriefly, microservices is a service-oriented software architecture in which server-side applications are constructed by combining many single-purpose, low-footprint network services. The touted benefits are improved modularity, reduced testing burden, better functional composition, environmental isolation, and development team autonomy. The opposite is a Monolithic architecture, where a large amount of functionality lives in a single service which is tested, deployed, and scaled as a single unit.\n\nIn early 2017 we reached a tipping point with a core piece of Segment’s product. It seemed as if we were falling from the microservices tree, hitting every branch on the way down. Instead of enabling us to move faster, the small team found themselves mired in exploding complexity. Essential benefits of this architecture became burdens. As our velocity plummeted, our defect rate exploded.\n\nEventually, the team found themselves unable to make headway, with 3 full-time engineers spending most of their time just keeping the system alive. Something had to change. This post is the story of how we took a step back and embraced an approach that aligned well with our product requirements and needs of the team.\n\n### Why Microservices work worked\nSegment’s customer data infrastructure ingests hundreds of thousands of events per second and forwards them to partner APIs, what we refer to as server-side destinations. There are over one hundred types of these destinations, such as Google Analytics, Optimizely, or a custom webhook. \n\nYears back, when the product initially launched, the architecture was simple. There was an API that ingested events and forwarded them to a distributed message queue. An event, in this case, is a JSON object generated by a web or mobile app containing information about users and their actions. A sample payload looks like the following:\n\n```json\n{\n  \"type\": \"identify\",\n  \"traits\": {\n    \"name\": \"Alex Noonan\",\n    \"email\": \"anoonan@segment.com\",\n    \"company\": \"Segment\",\n    \"title\": \"Software Engineer\"\n  },\n  \"userId\": \"97980cfea0067\"\n}\n```\nAs events were consumed from the queue, customer-managed settings were checked to decide which destinations should receive the event. The event was then sent to each destination’s API, one after another, which was useful because developers only need to send their event to a single endpoint, Segment’s API, instead of building potentially dozens of integrations. Segment handles making the request to every destination endpoint.\n\nIf one of the requests to a destination fails, sometimes we’ll try sending that event again at a later time. Some failures are safe to retry while others are not. Retry-able errors are those that could potentially be accepted by the destination with no changes. For example, HTTP 500s, rate limits, and timeouts. Non-retry-able errors are requests that we can be sure will never be accepted by the destination. For example, requests which have invalid credentials or are missing required fields.\n![](https://assets.contents.io/asset_QhV9GV3m.png)\n\nAt this point, a single queue contained both the newest events as well as those which may have had several retry attempts, across all destinations, which resulted in  [head-of-line](https://en.wikipedia.org/wiki/Head-of-line_blocking)blocking. Meaning in this particular case, if one destination slowed or went down, retries would flood the queue, resulting in delays across all our destinations.\n\nImagine destination X is experiencing a temporary issue and every request errors with a timeout. Now, not only does this create a large backlog of requests which have yet to reach destination X, but also every failed event is put back to retry in the queue. While our systems would automatically scale in response to increased load, the sudden increase in queue depth would outpace our ability to scale up, resulting in delays for the newest events. Delivery times for all destinations would increase because destination X had a momentary outage. Customers rely on the timeliness of this delivery, so we can’t afford increases in wait times anywhere in our pipeline.\n\n![](https://assets.contents.io/asset_joMYeSNt.png)\n\nTo solve the head-of-line blocking problem, the team created a separate service and queue for each destination. This new architecture consisted of an additional router process that receives the inbound events and distributes a copy of the event to each selected destination. Now if one destination experienced problems, only it’s queue would back up and no other destinations would be impacted. This microservice-style architecture isolated the destinations from one another, which was crucial when one destination experienced issues as they often do.\n\n![](https://assets.contents.io/asset_ElKl4CZv.png)\n\n### The Case for Individual Repos\nEach destination API uses a different request format, requiring custom code to translate the event to match this format. A basic example is destination X requires sending birthday as `traits.dob` in the payload whereas our API accepts it as `traits.birthday`. The transformation code in destination X would look something like this:\n\n```javascript\nconst traits = {}\ntraits.dob = segmentEvent.birthday\n```\n\nMany modern destination endpoints have adopted Segment’s request format making some transforms relatively simple. However, these transforms can be very complex depending on the structure of the destination’s API. For example, for some of the older and most sprawling destinations, we find ourselves shoving values into hand-crafted XML payloads.\n\nInitially, when the destinations were divided into separate services, all of the code lived in one repo. A huge point of frustration was that a single broken test caused tests to fail across all destinations. When we wanted to deploy a change, we had to spend time fixing the broken test even if the changes had nothing to do with the initial change. In response to this problem, it was decided to break out the code for each destination into their own repos. All the destinations were already broken out into their own service, so the transition was natural.\n\nThe split to separate repos allowed us to isolate the destination test suites easily. This isolation allowed the development team to move quickly when maintaining destinations.\n\n### Scaling Microservices and Repos\nAs time went on, we added over 50 new destinations, and that meant 50 new repos. To ease the burden of developing and maintaining these codebases, we created shared libraries to make common transforms and functionality, such as HTTP request handling, across our destinations easier and more uniform.\n\nFor example, if we want the name of a user from an event, `event.name()`can be called in any destination’s code. The shared library checks the event for the property key `name` and `Name`. If those don’t exist, it checks for a first name, checking the properties `firstName`, `first_name`, and `FirstName`. It does the same for the last name, checking the cases and combining the two to form the full name.\n\n```javascript\nIdentify.prototype.name = function() {\n  var name = this.proxy('traits.name');\n  if (typeof name === 'string') {\n    return trim(name)\n  }\n  \n  var firstName = this.firstName();\n  var lastName = this.lastName();\n  if (firstName && lastName) {\n    return trim(firstName + ' ' + lastName)\n  }\n}\n```\n\nThe shared libraries made building new destinations quick. The familiarity brought by a uniform set of shared functionality made maintenance less of a headache.\n\nHowever, a new problem began to arise. Testing and deploying changes to these shared libraries impacted all of our destinations. It began to require considerable time and effort to maintain. Making changes to improve our libraries, knowing we’d have to test and deploy dozens of services, was a risky proposition. When pressed for time, engineers would only include the updated versions of these libraries on a single destination’s codebase.\n\nOver time, the versions of these shared libraries began to diverge across the different destination codebases. The great benefit we once had of reduced customization between each destination codebase started to reverse. Eventually, all of them were using different versions of these shared libraries. We could’ve built tools to automate rolling out changes, but at this point, not only was developer productivity suffering but we began to encounter other issues with the microservice architecture.\n\nThe additional problem is that each service had a distinct load pattern. Some services would handle a handful of events per day while others handled thousands of events per second. For destinations that handled a small number of events, an operator would have to manually scale the service up to meet demand whenever there was an unexpected spike in load.\n\nWhile we did have auto-scaling implemented, each service had a distinct blend of required CPU and memory resources, which made tuning the auto-scaling configuration more art than science.\n\nThe number of destinations continued to grow rapidly, with the team adding three destinations per month on average, which meant more repos, more queues, and more services. With our microservice architecture, our operational overhead increased linearly with each added destination. Therefore, we decided to take a step back and rethink the entire pipeline.\n\n### Ditching Microservices and Queues\nThe first item on the list was to consolidate the now over 140 services into a single service. The overhead from managing all of these services was a huge tax on our team. We were literally losing sleep over it since it was common for the on-call engineer to get paged to deal with load spikes.\n\nHowever, the architecture at the time would have made moving to a single service challenging. With a separate queue per destination, each worker would have to check every queue for work, which would have added a layer of complexity to the destination service with which we weren’t comfortable. This was the main inspiration for Centrifuge. Centrifuge would replace all our individual queues and be responsible for sending events to the single monolithic service.\n\n![](https://assets.contents.io/asset_a0ViVzT6.png)\n\n### Moving to a Monorepo\nGiven that there would only be one service, it made sense to move all the destination code into one repo, which meant merging all the different dependencies and tests into a single repo. We knew this was going to be messy.\n\nFor each of the 120 unique dependencies, we committed to having one version for all our destinations. As we moved destinations over, we’d check the dependencies it was using and update them to the latest versions. We fixed anything in the destinations that broke with the newer versions.\n\nWith this transition, we no longer needed to keep track of the differences between dependency versions. All our destinations were using the same version, which significantly reduced the complexity across the codebase. Maintaining destinations now became less time consuming and less risky.\n\nWe also wanted a test suite that allowed us to quickly and easily run all our destination tests. Running all the tests was one of the main blockers when making updates to the shared libraries we discussed earlier.\n\nFortunately, the destination tests all had a similar structure. They had basic unit tests to verify our custom transform logic was correct and would execute HTTP requests to the partner’s endpoint to verify that events showed up in the destination as expected.\n\nRecall that the original motivation for separating each destination codebase into its own repo was to isolate test failures. However, it turned out this was a false advantage. Tests that made HTTP requests were still failing with some frequency. With destinations separated into their own repos, there was little motivation to clean up failing tests. This poor hygiene led to a constant source of frustrating technical debt. Often a small change that should have only taken an hour or two would end up requiring a couple of days to a week to complete.\n\n### Building a Resilient Test Suite\nThe outbound HTTP requests to destination endpoints during the test run was the primary cause of failing tests. Unrelated issues like expired credentials shouldn’t fail tests. We also knew from experience that some destination endpoints were much slower than others. Some destinations took up to 5 minutes to run their tests. With over 140 destinations, our test suite could take up to an hour to run.\n\nTo solve for both of these, we created Traffic Recorder. Traffic Recorder is built on top of yakbak, and is responsible for recording and saving destinations’ test traffic. Whenever a test runs for the first time, any requests and their corresponding responses are recorded to a file. On subsequent test runs, the request and response in the file is played back instead requesting the destination’s endpoint. These files are checked into the repo so that the tests are consistent across every change. Now that the test suite is no longer dependent on these HTTP requests over the internet, our tests became significantly more resilient, a must-have for the migration to a single repo.\n\nI remember running the tests for every destination for the first time, after we integrated Traffic Recorder. It took milliseconds to complete running the tests for all 140+ of our destinations. In the past, just one destination could have taken a couple of minutes to complete. It felt like magic.\n\n### Why a Monolith works\nOnce the code for all destinations lived in a single repo, they could be merged into a single service. With every destination living in one service, our developer productivity substantially improved. We no longer had to deploy 140+ services for a change to one of the shared libraries. One engineer can deploy the service in a matter of minutes.\n\nThe proof was in the improved velocity. In 2016, when our microservice architecture was still in place, we made 32 improvements to our shared libraries. Just this year we’ve made 46 improvements. We’ve made more improvements to our libraries in the past 6 months than in all of 2016.\n\nThe change also benefited our operational story. With every destination living in one service, we had a good mix of CPU and memory-intense destinations, which made scaling the service to meet demand significantly easier. The large worker pool can absorb spikes in load, so we no longer get paged for destinations that process small amounts of load.\n\n### Trade Offs\nMoving from our microservice architecture to a monolith overall was huge improvement, however, there are trade-offs:\n\n* 1、Fault isolation is difficult. With everything running in a monolith, if a bug is introduced in one destination that causes the service to crash, the service will crash for all destinations. We have comprehensive automated testing in place, but tests can only get you so far. We are currently working on a much more robust way to prevent one destination from taking down the entire service while still keeping all the destinations in a monolith.\n\n* 2、In-memory caching is less effective. Previously, with one service per destination, our low traffic destinations only had a handful of processes, which meant their in-memory caches of control plane data would stay hot. Now that cache is spread thinly across 3000+ processes so it’s much less likely to be hit. We could use something like Redis to solve for this, but then that’s another point of scaling for which we’d have to account. In the end, we accepted this loss of efficiency given the substantial operational benefits.\n\n* 3、Updating the version of a dependency may break multiple destinations. While moving everything to one repo solved the previous dependency mess we were in, it means that if we want to use the newest version of a library, we’ll potentially have to update other destinations to work with the newer version. In our opinion though, the simplicity of this approach is worth the trade-off. And with our comprehensive automated test suite, we can quickly see what breaks with a newer dependency version.\n\n### Conclusion\nOur initial microservice architecture worked for a time, solving the immediate performance issues in our pipeline by isolating the destinations from each other. However, we weren’t set up to scale. We lacked the proper tooling for testing and deploying the microservices when bulk updates were needed. As a result, our developer productivity quickly declined.\n\nMoving to a monolith allowed us to rid our pipeline of operational issues while significantly increasing developer productivity. We didn’t make this transition lightly though and knew there were things we had to consider if it was going to work.\n\n* 1、We needed a rock solid testing suite to put everything into one repo. Without this, we would have been in the same situation as when we originally decided to break them apart. Constant failing tests hurt our productivity in the past, and we didn’t want that happening again.\n\n* 2、We accepted the trade-offs inherent in a monolithic architecture and made sure we had a good story around each. We had to be comfortable with some of the sacrifices that came with this change.\n\nWhen deciding between microservices or a monolith, there are different factors to consider with each. In some parts of our infrastructure, microservices work well but our server-side destinations were a perfect example of how this popular trend can actually hurt productivity and performance. It turns out, the solution for us was a monolith.\n\n\n\nThe transition to a monolith was made possible by  [Stephen Mathieson](https://github.com/stephenmathieson), [Rick Branson](https://github.com/rbranson), [Achille Roussel](https://github.com/achille-roussel), [Tom Holmes](https://github.com/tsholmes), and many more.\n\nSpecial thanks to  [Rick Branson](https://github.com/rbranson)for helping review and edit this post at every stage.\n\n","tags":["Microservices"]},{"title":"处理Excel的框架 Jxls 2.6.0 简单笔记。","url":"/2019/05/23/45fa1046-1905-454c-99d2-7b944d35b378/","content":"### 1、概述\n根据业务的需要，项目中使用的 `easy-poi` 不能满足一些特殊的 `Excel` 表格的导出，就希望使用 `Jxls` 来处理， `Jxls` 从原生的 `Apache POI` 演进而来，灵活性高，但是封装通用起来，需要大量写方法，来应对各种情况。本文笔记描述在项目中使用的过程。\n<!--more-->\n### 2、项目中使用\n\n","tags":["Jxls"]},{"title":"Java使用RPC的方式连接SAP,从SAP获取数据","url":"/2019/05/23/d31361ba-98d4-4f1b-8ec5-28726292d21b/","content":"### 1、概要\n`EHR业务系统`需要定期从`SAP`系统库中获取数据。定时任务使用`XXL-Job`。SAP的lib库不是开源的，需要下载完之后，手动打进去。其中，配置SAP的环境变量也一件复杂的事情。\n<!-- more -->\n### 2、\n","tags":["SAP"]},{"title":"Java中关于数据去重、GROUP BY语句的使用","url":"/2019/05/19/5d2ff3d0-1861-46fa-a47d-9b9650c613e9-md/","content":"## 1、概要\n之前写过一个生日跨年查询的，后来的业务中，遇到一个根据业务范围需要去重的、还有根据一个去查询的操作。这篇笔记描述出业务场景和如何去解决的（maybe存在更好的解决方案）。\n<!-- more -->\n## 2、业务描述\n举例说明其中一个。"},{"title":"JHipster笔记之总览和网关(翻译)","url":"/2019/03/28/285fdec4-ba38-4e57-b655-6fe7edb1e6a8/","content":"### 1、概要\n首先自己理解中的JHipster，是一个开发平台，用于生成、开发、部署微服务项目；这篇笔记是在官网学习过程中的记录，基本是依靠自己的理解翻译过来的，其中有部分是自己的理解，记录再次，方便自己理解。\n<!--more-->\n### 2、通过JHipster使用微服务\n#### 2.1 微服务架构和单体架构\n第一个问题，想逆向生成一个单体应用or一个微服务结构的应用？JHipster会给你两个选择；\n* 单体架构，使用单独的、多适用性（one-size-fits-all application），同时包含前端的Angular代码和后盾的Spring Boot代码。\n* 微服务架构，切分了前台和后端，使得应用更加易于伸缩和扩展，解决基础设施问题。\n\n一个单体应用，更加容易工作，不需要其他更多的特别要求，这是我们的推荐操作，也是默认的。\n#### 2.2、微服务架构总览\nJHipster的微服务架构的工作方式如下：\n* 网关是一个JHipster-generated的应用（逆向产生），它来处理网络的流量（handles Web traffic），并提供Angular应用程序。如果您遵循前后端的模式，可以有多个网关（非强制要求）。\n* Traefik是一个HTTP反向代理模型、负载均衡器，可以与网关一起使用。\n* JHipster Registry是一个运行时程序，所有的注册者可以从这个注册中心获取到他们的配置，还可以提供运行时的镜像控制面板。\n* Consul是发现服务，和Eureka注册中心是一样，采用KV存储方式；它可以作为代替品使用。\n* JHipster UAA全称：JHipster-based User Authentication and Authorization system，使用的是OAuth2.0协议。\n* Microservices是使用JHipster-generated应用逆向生成的，处理REST请求，它们多个实例可以并行，处理大量的负载请求。\n* JHipster Console是一个基于ELK堆栈的监视和警报控制台。\n\n在下面的这个图中，绿色组件是特定于应用程序的，蓝色组件提供其底层基础设施。\n![](../../../../../../images/2019032802.png)\n### 3、网关\nJHipster可以逆向出API网关。网关是一个正常的JHipster应用，可以在项目中使用普通的JHipster选项和开发工作流，它开一个作为一个程序的入口。更具体一些，它为所有的微服务提供了HTTP路由、负载均衡、服务质量、安全性和API文档。\n#### 3.1、概要提示\n1.架构图\n2.HTTP路由\n3.安全性\n4.自动文档\n5.速率限制\n6.访问控制策略\n#### 3.2、架构图\n如下图所示：\n\n![](../../../../../../images/3.png)\n\n分析:\n#### 3.3、HTTP使用网关进行路由请求\n当网关和微服务运行时，他们会注册自己到注册中心上。网关会自动代理所有的请求到微服务上，使用名字即可。\n例如：你过你的网关运行是：<b>http://localhost:8080</b>，你可以通过<b>http://localhost:8080/app1/rest/foos</b>来获取foos来获取app1提供的服务；如果您尝试使用浏览器做到这中操作，不要忘记，REST资源在JHipster中默认是安全的，所以你需要正确的JWT头部信息来实现，或者删除微服务中的<b>MicroserviceSecurityConfiguration</b>这个Java类中的这些URL的安全性。\n如果一个服务运行着多个实例：网关将会总注册表中获取这些实例：\n* 使用Netflix Ribbon处理HTTP请求的负载均衡\n* 使用Netflix Hystrix熔断器，当实例崩溃会被安全的、快速的移除\n#### 3.4、安全\n略。\n#### 3.5、JWT\nJWT的简介........\n为了能安全考虑，一个JWT的密钥令牌必须在所有的应用之间共享；\n#### 3.6、速率限制\n这是一个高级功能；使用[Bucket4j](https://github.com/vladimir-bukhtoyarov/bucket4j)和[Hazelcast](https://hazelcast.com/)来给微服务提供服务。\n网关提供速率限制的功能，所以REST可以限制请求的数量。\n* 通过IP限制(针对匿名用户)\n* 通过用户登录限制(针对已经登录的用户)\nJHipster\n#### 3.7、访问控制策略\n默认情况，所有的微服务注册者都经过网关使用。如果你想把一个特别的API不经过网关，你可以使用网关的特殊访问控制策略过滤，它可以在<b>application-*.yml</b>使用<b>jhipster.gateway.authorized-microservices-endpoints</b>这个属性来配置：\n```java\njhipster:\n    gateway:\n        authorized-microservices-endpoints: # Access Control Policy, if left empty for a route, all endpoints will be accessible\n            app1: /api,/v2/api-docs # recommended dev configuration\n```\n例如，如果你想只让<b>/api/foo</b>微服务端点<b>bar</b>可用：\n```java\njhipster:\n    gateway:\n        authorized-microservices-endpoints:\n            bar: /api/foo\n```","tags":["JHipster"]},{"title":"Spring中的bean-配置与注入","url":"/2019/03/28/e4d4efd7-c875-4b69-921f-cb6299ef7e72/","content":"### 1、概要\nSpring中的bean配置和注入，需要理解透彻，但是没有去总结的很全面，在cnblog.com发现一篇文章，作者总结的很全面，原文地址：[https://www.cnblogs.com/wuchanming/p/5426746.html](https://www.cnblogs.com/wuchanming/p/5426746.html),本文的笔记只是挑出来个人认为重要的部分。\n<!--more-->\n### 2、bean的配置\nBean的配置信息定义了Bean的实现和依赖关系，Spring容器根据各种形式的Bean配置信息在容器内部建立bean定义注册表，然后根据注册表加载、实例化Bean，并建立Bean和Bean的依赖关系，最后将这些准备就绪的Bean放到Bean缓存池中，以供外层的应用程序进行调用。\n##### Bean的配置有三种方法：\n①、基于XML来配置Bean；\n②、使用注解定义Bean；\n③、基于Java类提供Bean定义信息；\n#### 2.1、基于XML来配置Bean\n放一张截图：\n![](../../../../../../images/2019032801.png)\n\n#### 2.2、使用注解定义Bean\nSpring容器成功启动的三大要件分别是：①Bean定义信息、②Bean实现类和③Spring本身；如果采用基于XML的配置，Bean定义信息和Bean实现类本身是分离的；而采用基于注解的配置方式时，<b>Bean定义信息即通过在Bean实现类上标注注解实现<b>。\n```java\npackage com.baobaotao.anno;\n\nimport org.springframework.stereotype.Component;\nimport org.springframework.stereotype.Repository;\n//①通过Repository定义一个DAO的Bean\n\n@Component(\"userDao\")\npublic class UserDao {\n\n}\n```\n在①处，我们使用@Component注解在UserDao类声明处对类进行标记，它可以被Spring容器识别，Spring容器自动将POJO转换为容器管理的Bean。\n它和XML是等效的：\n```java\n<bean id=\"UserDao\" class=\"com.baobaotao.anno.UserDao\"><bean/>\n```\n\n除了@Component以外，Spring提供了3个功能基本和@Component等效的注解，他们分别应用于DAO、Service以及Web层的Controller的注解；所以也成这些注解为Bean的衍生注解；\n①、@Repository:用于对DAO实现类进行标注；\n②、@Service:用于对Service实现类进行标注；\n③、@Controller:用于对Controller实现类进行标注；\n之所以要在@Component之外提供这三个特殊的注解，除了让注解类本身的用途更加清晰明白，此外Spring将赋予他们一些特殊的功能；\n","tags":["Spring"]},{"title":"Java注解笔记","url":"/2019/03/27/18778999-e0d4-4696-9462-88ca98eff534/","content":"### 1、概述\n1.1、开发过程中，遇到一些元注解或者常见的注解，本笔记记录一下这些注解的作用以及部分注解的原理。\n<!--more-->\n### 2、元注解\n元注解可以修饰注解，是注解的注解，叫做元注解；\n#### 2.1、@Retention\n作用:需要在什么级别保存该注释信息，用于描述注解的声明周期。\n在修饰注解的时候，我们通常这样用：\n```java\n@Retention(RetentionPolicy.RUNTIME)\n```\n其中，注解@Retention有一个属性value，是RetentionPolicy类型的；RetentionPolicy有三个值，分别为：CLASS、RUNTIME、SOURCE;\n按照声明周期划分为三类：\n①、RetentionPolicy.SOURCE:注解只保留在源文件，当Java文件编译成.class文件的时候，注解被遗弃；\n②、RetentionPolicy.CLASS:注解被保留到.class文件中，但是JVM加载.class文件的时候被遗弃，这是默认的生命周期；\n③、RetentionPolicy.RUNTIME:注解不仅被保留到.class文件中，JVM加载.class文件之后，仍然存在。\n#### 2.2 @Target\n作用:用于描述注解的使用范围。\n下面是部分源代码：\n```java\n@Documented\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.ANNOTATION_TYPE)\npublic @interface Target {\n    /**\n     * Returns an array of the kinds of elements an annotation type\n     * can be applied to.\n     * @return an array of the kinds of elements an annotation type\n     * can be applied to\n     */\n    ElementType[] value();\n}\n```\n其中，ElementType的取值有：TYPE、FIELD、METHOD等;下面是ElementType的源代码：\n```java\n\n/**\n * The constants of this enumerated type provide a simple classification of the\n * syntactic locations where annotations may appear in a Java program. These\n * constants are used in {@link Target java.lang.annotation.Target}\n * meta-annotations to specify where it is legal to write annotations of a\n * given type.\n *\n * <p>The syntactic locations where annotations may appear are split into\n * <em>declaration contexts</em> , where annotations apply to declarations, and\n * <em>type contexts</em> , where annotations apply to types used in\n * declarations and expressions.\n *\n * <p>The constants {@link #ANNOTATION_TYPE} , {@link #CONSTRUCTOR} , {@link\n * #FIELD} , {@link #LOCAL_VARIABLE} , {@link #METHOD} , {@link #PACKAGE} ,\n * {@link #PARAMETER} , {@link #TYPE} , and {@link #TYPE_PARAMETER} correspond\n * to the declaration contexts in JLS 9.6.4.1.\n *\n * <p>For example, an annotation whose type is meta-annotated with\n * {@code @Target(ElementType.FIELD)} may only be written as a modifier for a\n * field declaration.\n *\n * <p>The constant {@link #TYPE_USE} corresponds to the 15 type contexts in JLS\n * 4.11, as well as to two declaration contexts: type declarations (including\n * annotation type declarations) and type parameter declarations.\n *\n * <p>For example, an annotation whose type is meta-annotated with\n * {@code @Target(ElementType.TYPE_USE)} may be written on the type of a field\n * (or within the type of the field, if it is a nested, parameterized, or array\n * type), and may also appear as a modifier for, say, a class declaration.\n *\n * <p>The {@code TYPE_USE} constant includes type declarations and type\n * parameter declarations as a convenience for designers of type checkers which\n * give semantics to annotation types. For example, if the annotation type\n * {@code NonNull} is meta-annotated with\n * {@code @Target(ElementType.TYPE_USE)}, then {@code @NonNull}\n * {@code class C {...}} could be treated by a type checker as indicating that\n * all variables of class {@code C} are non-null, while still allowing\n * variables of other classes to be non-null or not non-null based on whether\n * {@code @NonNull} appears at the variable's declaration.\n *\n * @author  Joshua Bloch\n * @since 1.5\n * @jls 9.6.4.1 @Target\n * @jls 4.1 The Kinds of Types and Values\n */\npublic enum ElementType {\n    /** Class, interface (including annotation type), or enum declaration */\n    TYPE,\n\n    /** Field declaration (includes enum constants) */\n    FIELD,\n\n    /** Method declaration */\n    METHOD,\n\n    /** Formal parameter declaration */\n    PARAMETER,\n\n    /** Constructor declaration */\n    CONSTRUCTOR,\n\n    /** Local variable declaration */\n    LOCAL_VARIABLE,\n\n    /** Annotation type declaration */\n    ANNOTATION_TYPE,\n\n    /** Package declaration */\n    PACKAGE,\n\n    /**\n     * Type parameter declaration\n     *\n     * @since 1.8\n     */\n    TYPE_PARAMETER,\n\n    /**\n     * Use of a type\n     *\n     * @since 1.8\n     */\n    TYPE_USE\n}\n```\nTYPE_PARAMETER和TYPE_USE是1.8版本有的；\n其中，\n* TYPE:类、接口（包括注解类型）和枚举类型的声明；\n* FIELD:字段声明（包括枚举常量）；\n* METHOD:方法声明；\n* PARMETER:参数声明；\n* CONSTRUCTOR:构造函数声明；\n* LOCAL_VARIABLE: 本地变量声明；\n* ANNOTATION_TYPE:注解类型声明；\n* PACKAGE:包声明；\n* TYPE_PARAMETER:类型参数声明，类的泛型声明；\n* TYPE_USE:包括类型声明和类型参数声明，方便进行类型检查；（还没有试过）；\n\n","tags":["Java"]},{"title":"开发中的解耦与复用","url":"/2019/03/22/51cf8560-d04b-4fbd-a04b-d40cc261ca36/","content":"### 概要\n在设计<b>微服务</b>模式的开发系统的时候，按照实际的划分规则进行模块区分，注册中心、网关、权限校验、字段服务、链路追踪、日志管理等等，还需要将共用的类或者接口单独取出来，放到一个模块中。\n<!--more-->\n### 总结\n1、公共的类，汇总到一起\n2、引入的依赖、.jar包，考虑是否放到公共类中进行复用","tags":["总结"]},{"title":"Spring Cloud核心组件笔记","url":"/2019/03/18/d27015fb-408b-4e82-9827-ec8d7acdd75d/","content":"### 1、概要\n学习微服务模式开发，记录一下对每个组件的理解和笔记；不断按照最新的理解进行修改并补充。本文笔记的Spring Cloud的核心组件包括：Eureka、Ribbon、Zuul、Hystrix、Feign组件。\n<!-- more -->\n### 2、Eureka\n* Eureka Server:注册中心，里面有一个注册表，保存了各个服务所在的机器和端口号；\n* Eureka Client:负责将这个服务的信息注册到Eureka Server中；\n\n### 3、Feign\n* 例如，订单模块调用积分模块，现在订单模块已经知道了积分模块的地址和端口号了。发出请求就需要借助Feign；\n* Feign根据指定的服务进行建立链接、构造请求、发起请求、获取响应、解析响应等；\n* 上述的操作，Feign的一个机制：动态代理；\n\n\n### 4、Ribbon\n* 服务模块有了地址、端口、和建立连接请求的条件，如果存在多个同行服务在不同机器上，比如：\n  *  172.1.16.58:9000\n  *  172.1.16.59:9000\n  *  172.1.16.60:9000\n  *  172.1.16.61:9000\n* Ribbon的作用是负载均衡。每次请求的时候，会通过不同的算法均匀的将请求分发到各个服务上；\n* Ribbon会从Eureka Client获取到对应的服务注册表，相对应的知道了服务部署的地址和监听的端口；\n* Ribbon可以使用默认的轮询算法，从其中选择一台机器；\n* Feign就会针对Ribbon选中的这台机器，进行下一步动作；\n* Feign默认是带有Ribbon的依赖的，开发的时候不需要单独引入；\n\n### 5、Hystrix\n* 熔断机制。隔离、熔断、降级；Hystrix会生成很多小的线程池，比如订单的试一个线程池、积分是一个线程池；每个线程池中仅仅用于请求的服务；\n* 即使积分服务挂掉，订单服务的线程池是正常的，仍旧可以工作，不会受到影响；\n* 熔断：积分服务挂掉的话，设置时间5分钟内够来的请求直接返回；\n* 降级：需要对某用户的积分进行操作，但是积分服务挂掉没办法进行。这时候可以设计一个专门存放故障的数据库；来记录对这个用户的积分操作是什么样子的。等积分服务恢复之后，可以手动还原回去；\n\n### 6、Zuul\n* 使用了一个网关，不需要关心后端有多少个微服务，只需要知道网关的地址即可。所有的请求都请过网关进行处理；\n* 好处：可以做统一的降级、限流、认证授权等；","tags":["SpringCloud"]},{"title":"MyBatis映射文件中的注意事项","url":"/2019/03/15/64e17dff-f1e7-4527-a12c-8f7e5bd1e064/","content":"### 1、概要\n在写MyBatis的xml映射文件的时候，有一些需要注意的点：\n<!-- more -->\n### 2、具体内容：\n* ResultMap中子标签```<result/>```标签中的```property```和```column```对应字段必须所在的```entity```实体类中；\n* 如果```entity```实体类中存在，但是没有写在```ResultMap```中，则不显示；\n* ```<if>```标签中存在```if```条件，如果外层关联过许多表的话，查询出的数据结果集可能会用重复，因此，限制条件必须要放到外层，而不是写在```<if>```标签中；\n* 在```Navicate```中的```union all```语句是能够正确查询的，但是放到```mapper```映射文件中，需要加一层，```select a.* from (...) a```；\n* MyBatis中不支持通常写法的大于号和小于号，这里需要转义：大于号：```&gt;```，小于号：```&lt;```；","tags":["MyBatis"]},{"title":"Git笔记及理解(不断补充)","url":"/2019/03/12/517e01df-a412-4fb6-8386-9d91574035f6/","content":"### 1、概要\n作为团队开发的一项重要利器，使用好Git是一件重要的事情；提高协作效率，节约协作时间。\n<!-- more -->\n### 2、基础知识\n#### 2.1 环境的搭建\n#### 2.2 推送第一个文件至GitHub代码仓库\n#### 2.3 原理\n### 3、命令行维护\n#### 3.1 \n","tags":["Git"]},{"title":"开发规范笔记","url":"/2019/03/07/16d9fb52-dd5b-4339-9b38-a98df48a8b46/","content":"### 概要\n日常开发中，虽然代码可用性通过，要求的功能实现了，但是有一些规范需要去遵守；避免不注意犯了一些低级错误；本文笔记梳理一些常忽略的点，以免下次再进坑；\n<!--more-->\n1、公共类中的方法最好不要动；\n2、uuid要谨慎使用；\n3、公共的方法要放到公共的类中；\n4、业务类的方法比较多，取名字要规范一些，长点不要紧，但是一定要读出来方法的作用是什么；如果表达不出来，加上必要的注释；\n5、尽量不要捕获类似Exception通用异常，而是捕获特定的异常，具体到哪一种子类型；\n6、在复杂的生产系统中，标准出错STERR不是合适的输出选项，尤其是分布式系统，最好使用产品日志；详细的输出到日志系统中；STREE无法找到堆栈轨迹；\n不要在<b>catch</b>代码块中直接[printStackTrace()](https://docs.oracle.com/javase/9/docs/api/java/lang/Throwable.html#printStackTrace--)\n7、[配置文件]在微服务的resources下的配置文件，可能会有许多的配置文件，每个配置文件尽量要求做到名字直观易懂，必要时在配置文件中做出单独的说明；","tags":["总结"]},{"title":"前后端开发中对数据类型的理解和笔记","url":"/2019/03/07/f34c66e6-28df-4cc4-bac1-ac4e0cf6bb6a/","content":"### 1、概要\n总结一下基础知识，关于最近前后端的开发中，对基础知识点的理解和重新认识；主要包括Java的List/Map、以及JSON格式的数据与数组转换的总结；\n<!--more-->\n### 2、背景\n首先根据开发中的两个需求，描述记录；\n1、在EHR系统开发中，对员工的特种作业证书进行筛选统计，筛选规则：将同一证书类型、同一证书名字、同一登记时间的，取变动时间最新的一条数据展示；\n2、前台报表显示列项目，前端Vue使用ElementUI的穿梭器实现；用户可以[自定义显示]要显示的列项目；每个用户的设置不一样；\n### 3、EHR中的解决描述记录\n##### 3.1 思路和具体实现\n员工的特种证书表中，冗余的数据太多，维护的太乱，一个证书可能有多次变动时间；数据的来源是从SAP生产800数据库中拉取过来，最新的记录没有标记；为了取数准确，每为员工的数据分两步取数：\n1、先取出<b>不包括</b>在【同一证书名称、类型、登记时间】这个条件的数据。用<b>GROUP BY</b>分组语句查询出来；\n2、再取出范围在【同一证书名称、类型、登记时间】这个条件的数据；\n看下表结构（仅仅罗列了关键的字段）：\n\n| 字段名称 | 类型| 备注 |\n| ------ | ------ | ------ |\n| employee_special_uuid | varchar | uuid |\n| employee_id | varchar | 员工ID |\n| special_operations_type | varchar | 证书类型 |\n| special_operations_project | varchar | 证书项目（名字）|\n| special_operations_record_date | date | 登记日期 |\n| sp_begin_date | date | 变动日期 |\n| sp_end_date | date | 结束日期 |\n\n\n```shell\nSELECT\n\t t.employee_id  ,\n\t t.special_operations_type  ,\n\t t.special_operations_project  ,\n\t t.special_operations_cert_code  ,\n\t t.special_operations_cert_date  ,\n\t t.special_operations_cert_end_date  ,\n\t t.special_operations_expire_date  ,\n\t t.special_operations_part_time  ,\n\t t.special_operations_status  ,\n\t t.special_operations_record_date  ,\n\t t.employee_special_uuid  ,\n\t t.special_operations_status_text  ,\n\t t.special_operations_part_time_text  ,\n\t t.special_operations_cert_org  ,\n\t t.special_operations_type_text  ,\n\t t.special_operations_project_text  ,\n\t t.sp_remark  ,\n\t t.sp_begin_date  ,\n\t t.sp_end_date\nFROM\n\tm_special_operations t\nWHERE\n\t(t.special_operations_type IN(\n\t\t(\n\t\t\tSELECT\n\t\t\t\tt.special_operations_type\n\t\t\tFROM\n\t\t\t\tm_special_operations t\n\t\t\tWHERE\n\t\t\t\tt.employee_id = #{employeeId}\n\t\t\tAND t.special_operations_status = '01'\n\t\t\tGROUP BY\n\t\t\t\tt.special_operations_type\n\t\t\tHAVING\n\t\t\t\tcount(t.special_operations_type) = 1\n\t\t)\n\t)\nor t.special_operations_project IN(\n\t(\n\t\tSELECT\n\t\t\tt.special_operations_project\n\t\tFROM\n\t\t\tm_special_operations t\n\t\tWHERE\n\t\t\tt.employee_id = #{employeeId}\n\t\tAND t.special_operations_status = '01'\n\t\tGROUP BY\n\t\t\tt.special_operations_project\n\t\tHAVING\n\t\t\tcount(t.special_operations_project) = 1\n\t)\n)\n\nor t.special_operations_record_date IN(\n\t(\n\t\tSELECT\n\t\t\tt.special_operations_record_date\n\t\tFROM\n\t\t\tm_special_operations t\n\t\tWHERE\n\t\t\tt.employee_id = #{employeeId}\n\t\tAND t.special_operations_status = '01'\n\t\tGROUP BY\n\t\t\tt.special_operations_record_date\n\t\tHAVING\n\t\t\tcount(t.special_operations_record_date) = 1\n\t)\n)\n)\n\nAND t.employee_id = #{employeeId}\nAND t.special_operations_status = '01'\ngroup by \n\t t.employee_id  ,\n\t t.special_operations_type  ,\n\t t.special_operations_project  ,\n\t t.special_operations_cert_code  ,\n\t t.special_operations_cert_date  ,\n\t t.special_operations_cert_end_date  ,\n\t t.special_operations_expire_date  ,\n\t t.special_operations_part_time ,\n\t t.special_operations_status  ,\n\t t.special_operations_record_date  ,\n\t t.employee_special_uuid  ,\n\t t.special_operations_status_text  ,\n\t t.special_operations_part_time_text  ,\n\t t.special_operations_cert_org  ,\n\t t.special_operations_type_text  ,\n\t t.special_operations_project_text  ,\n\t t.sp_remark  ,\n\t t.sp_begin_date  ,\n\t t.sp_end_date\n\norder by t.sp_begin_date desc\n```\n\n上述语句能够将<b>不包括</b>在【同一证书名称、类型、登记时间】这个条件的数据。<b>GROUP BY</b>的用法不记录笔记；\n<b>取出范围在【同一证书名称、类型、登记时间】这个条件的数据</b>只要稍微改动一下查询条件即可，这样，先把要求的数据集先取出来，然后在通过Java代码去处理List即可；\n<b>最后一个order by t.sp_begin_date desc其实用处很大的，group by已经分组了，然后将最新的时间排在了结果集的最上面，对于后边的List处理起到了很方便的作用；</b>\n```shell\nSELECT\n\t t.employee_id  ,\n\t t.special_operations_type  ,\n\t t.special_operations_project  ,\n\t t.special_operations_cert_code  ,\n\t t.special_operations_cert_date  ,\n\t t.special_operations_cert_end_date  ,\n\t t.special_operations_expire_date  ,\n\t t.special_operations_part_time  ,\n\t t.special_operations_status  ,\n\t t.special_operations_record_date  ,\n\t t.employee_special_uuid  ,\n\t t.special_operations_status_text  ,\n\t t.special_operations_part_time_text  ,\n\t t.special_operations_cert_org  ,\n\t t.special_operations_type_text  ,\n\t t.special_operations_project_text  ,\n\t t.sp_remark  ,\n\t t.sp_begin_date  ,\n\t t.sp_end_date\nFROM\n\tm_special_operations t\nWHERE\n\t(t.special_operations_type IN(\n\t\t(\n\t\t\tSELECT\n\t\t\t\tt.special_operations_type\n\t\t\tFROM\n\t\t\t\tm_special_operations t\n\t\t\tWHERE\n\t\t\t\tt.employee_id = #{employeeId}\n\t\t\tAND t.special_operations_status = '01'\n\t\t\tGROUP BY\n\t\t\t\tt.special_operations_type\n\t\t\tHAVING\n\t\t\t\tcount(t.special_operations_type) > 1\n\t\t)\n\t)\nand t.special_operations_project IN(\n\t(\n\t\tSELECT\n\t\t\tt.special_operations_project\n\t\tFROM\n\t\t\tm_special_operations t\n\t\tWHERE\n\t\t\tt.employee_id = #{employeeId}\n\t\tAND t.special_operations_status = '01'\n\t\tGROUP BY\n\t\t\tt.special_operations_project\n\t\tHAVING\n\t\t\tcount(t.special_operations_project) > 1\n\t)\n)\n\nand t.special_operations_record_date IN(\n\t(\n\t\tSELECT\n\t\t\tt.special_operations_record_date\n\t\tFROM\n\t\t\tm_special_operations t\n\t\tWHERE\n\t\t\tt.employee_id = #{employeeId}\n\t\tAND t.special_operations_status = '01'\n\t\tGROUP BY\n\t\t\tt.special_operations_record_date\n\t\tHAVING\n\t\t\tcount(t.special_operations_record_date) > 1\n\t)\n)\n)\n\nAND t.employee_id = #{employeeId}\nAND t.special_operations_status = '01'\ngroup by \n\t t.employee_id  ,\n\t t.special_operations_type  ,\n\t t.special_operations_project  ,\n\t t.special_operations_cert_code  ,\n\t t.special_operations_cert_date  ,\n\t t.special_operations_cert_end_date  ,\n\t t.special_operations_expire_date  ,\n\t t.special_operations_part_time ,\n\t t.special_operations_status  ,\n\t t.special_operations_record_date  ,\n\t t.employee_special_uuid  ,\n\t t.special_operations_status_text  ,\n\t t.special_operations_part_time_text  ,\n\t t.special_operations_cert_org  ,\n\t t.special_operations_type_text  ,\n\t t.special_operations_project_text  ,\n\t t.sp_remark  ,\n\t t.sp_begin_date  ,\n\t t.sp_end_date\n\norder by t.sp_begin_date desc\n```\n##### 3.2 Java代码的实现；\n先放上代码：\n```java\n    /**\n     * 获取资质信息（1、职称信息 2、职业资格 3、特种证书）\n     * @param employeeId\n     * */\n    @GetMapping(value = \"/getPostionTitleData/{employeeId}\")\n    @ResponseBody\n    public ObjectRestResponse getPostionTitleData(@PathVariable String employeeId) throws Exception{\n        List<MPostionTitle> list1 = mPostionTitleBiz.selectPostionTitleByEmpId(employeeId);\n\n        // 职业资格重复的记录\n        List<MProQualifications> mProQualificationsList = mProQualificationsBiz.selectProQuaById(employeeId);\n        // 职业资格不重复的记录；\n        List<MProQualifications> notRepeatMpqList = mProQualificationsBiz.selectMProQuaNotRepeat(employeeId);\n\n        // 特种证书重复的记录\n        List<MSpecialOperations> mSpecialOperationsList = mSpecialOperationsBiz.selectSpecOperationById(employeeId);\n        // 特种证书不重复的记录\n        List<MSpecialOperations> notRepeatMsoList = mSpecialOperationsBiz.selectSpecOPerationNotRepeat(employeeId);\n\n        // 处理职业资格重复的记录；\n        for (int i = 0; i < mProQualificationsList.size() - 1; ++i) {\n            DateFormat df = new SimpleDateFormat(\"yyyy-MM-dd\");\n            if (mProQualificationsList.get(i).getEmployeeProType().equals(mProQualificationsList.get(i + 1))\n                    && mProQualificationsList.get(i).getEmployeeProLevel().equals(mProQualificationsList.get(i + 1))\n                    && mProQualificationsList.get(i).getEmployeeProDate().equals(mProQualificationsList.get(i + 1))) {\n                if (mProQualificationsList.get(i).getEmployeeProDate().getTime() > mProQualificationsList.get(i + 1).getEmployeeProDate().getTime()) {\n                    mProQualificationsList.set((i + 1), mProQualificationsList.get(i));\n                }\n            }\n        }\n        // 去除重复的记录\n        for (int i = 0; i < mProQualificationsList.size() - 1; i++) {\n            for (int j = mProQualificationsList.size() - 1; j > i; j--) {\n                mProQualificationsList.remove(j);\n            }\n        }\n        mProQualificationsList.addAll(notRepeatMpqList);\n\n        // 处理特种证书重复的记录；\n        for (int i = 0; i < mSpecialOperationsList.size() - 1; ++i) {\n            DateFormat df = new SimpleDateFormat(\"yyyy-MM-dd\");\n            if (mSpecialOperationsList.get(i).getSpecialOperationsType().equals(mSpecialOperationsList.get(i + 1).getSpecialOperationsType())\n                    && mSpecialOperationsList.get(i).getSpecialOperationsProject().equals(mSpecialOperationsList.get(i + 1).getSpecialOperationsProject())\n                    && mSpecialOperationsList.get(i).getSpecialOperationsRecordDate().equals(mSpecialOperationsList.get(i + 1).getSpecialOperationsRecordDate())) {\n                if (mSpecialOperationsList.get(i).getSpBeginDate().getTime() > mSpecialOperationsList.get(i + 1).getSpBeginDate().getTime()) {\n                    mSpecialOperationsList.set((i + 1), mSpecialOperationsList.get(i));\n                }\n            }\n        }\n        // 去除重复的msList\n        for (int i = 0; i < mSpecialOperationsList.size() - 1; i ++) {\n            for (int j = mSpecialOperationsList.size() - 1;j > i; j-- ) {\n                if (mSpecialOperationsList.get(i).equals(mSpecialOperationsList.get(j))) {\n                    mSpecialOperationsList.remove(j);\n                }\n            }\n        }\n        mSpecialOperationsList.addAll(notRepeatMsoList);\n\n        Map map = new HashMap();\n        map.put(\"titleData\", list1);\n        map.put(\"proQualificationsData\", mProQualificationsList);\n        map.put(\"specialWorkData\", mSpecialOperationsList);\n\n        return new ObjectRestResponse().data(map);\n    }\n```\n\n拿特种证书来说，用for()循环，将List循环，for()的作用是将List中的元素全部设置为同一个元素，if条件语句是限定第i个元素和i+1个元素，如果相同，则是一样的，将i+1个元素设置为跟i一样，刚才上面说了，<b>最后一个order by t.sp_begin_date desc其实用处很大的，group by已经分组了，然后将最新的时间排在了结果集的最上面，对于List处理起到了很方便的作用；</b>处理之后的list，数据永远是符合要求的那一条；\n\n第二个for()循环的作用是出去list的重复值，其实只留下一个即可；用for()一个一个的remove()处理掉；这样剩余的就符合条件了；\n### 4、前台报表列显示与隐藏实现\n##### 4.1 思路和具体实现\n前台之前是用mock模拟实现的，这是存储的方式：\n```javascript\n  saveColumnSetting: config => {\n    localStorage.setItem('columns', JSON.stringify(JSON.parse(config.body).columns))\n    console.log('这是mock5')\n  },\n```\n存储到了localStorage中，如果浏览器清除缓存了，或者用户使用了另一个浏览器登录或者其他的电脑的登录，这个配置就会失效；\n这是用户打开页面加载的代码：\n```javascript\n  created() {\n    const model = 'projectApply';\n    getColumnSetting(model).then(response => {\n      if (response && response.length) {\n        this.columns = response\n      }\n    })\n  }\n```\n获取也是从localStorage中获取：\n```javascript\ngetColumnSetting: () => ({ data: JSON.parse(localStorage.getItem('columns')) })\n```\n这是向后传递的时候，调用的函数：\n```javascript\n    changeColumnWidth(newWidth, oldWidth, column) {\n      this.columns.forEach(el => {\n        if (el.code === column.property) {\n          el.width = newWidth\n        }\n      })\n      saveColumnSetting({ module: this.module, columns: JSON.stringify(this.columns) })\n    },\n```\ndata的内容是：\n```javascript\n{ module: this.module, columns: JSON.stringify(this.columns) }\n```\n其中用到了<b>JSON.stringify()</b>函数，将数组转换成JSON的格式，存储到后端的columns字段中；\n这是接口，函数传递过来的是data：\n```javascript\nexport function saveColumnSetting(data) {\n  return request({\n    url: '/api/crm/mdColumnsShow/saveColumnSetting',\n    method: 'post',\n    data\n  })\n}\n```\n<b>this.columns</b>是一个数组，可以使用forEach箭头函数将其遍历；\n那就需要将这部分的配置，存储到数据库中，下面是存储的表结构：\n\n| 字段名称 | 类型| 备注 |\n| ------ | ------ | ------ |\n| id | varchar | uuid |\n| module | varchar | 模块名称 |\n| user_id | varchar | 用户Id|\n| user_name | varchar | 用户名字 |\n| columns | date | 列的配置 |\n\n\n### 总结\n总结上述的过程，操作List;","tags":["总结"]},{"title":"MySQL常见问题及优化","url":"/2019/01/26/5a626726-4e1c-41ae-b704-8e4331860a5a/","content":"## MySQL的优化方案有哪一些？\n本文介绍一下优化SQL语句和优化索引。\n\n### 本文记录MySQL 本文梗概如下:\n* 优化SQL\n* 优化索引\n* 开启查询缓存\n<!-- more -->\n* 配合使用Redis\n* MySQL主从复制\n* 优化MySQL自带的分区表\n* 垂直拆分,分解大系统,成为小系统,分布式系统\n* 水平拆分\n* 硬件上的升级、存储引擎、MySQL本身的配置文件\n\n## （一）优化SQL\n#### 1、通过MySQL自有的优化语句\n优化SQL语句，通过脚本命令来了解执行率较低的语句，显示一下状态等。\n* SHOW命令\n  * SHOW Status可以了解SQL的执行频率。可以显示日志，显示特定的数据库、表、索引以及进程还有权限表中的信息等等。\n介绍一些常见的字段\n  * Innodb_rows_read:Select查询返回的行数。\n  * Innodb_rows_inserted:执行INSERT操作插入的行数。\n  * Innodb_rows_updated:执行UPDATE操作更新的行数。\n  * Innodb_rows_deleted:执行DELETE操作删除的行数。\n\n","tags":["MySQL"]},{"title":"MyBatis语法总结","url":"/2019/01/02/9ec1b26b-333e-4b8c-875c-4c20d6be8d9f/","content":"### 1、概述\n* MyBatis在一些情况下，与在MySQL里面直接查询两者的运行情况（此处无关结果集）不一定是一样的，有些语句在MyBatis中运行是报错的。\n<!-- more -->\n\n### 2、MyBatis中的<where></where>标签\n\n\n","tags":["MyBatis"]},{"title":"前端后端数据交互过程例解与调试","url":"/2018/12/29/205380c3-c733-4f1b-ba14-9bc6274e63ab/","content":"### 1、概述\n前后端分离项目进行数据交互的时候，出现错误改如何快速定位调试。\n<!-- more -->\n### 2、前后交互数据的过程","tags":["技术"]},{"title":"方案解决与思路提示","url":"/2018/12/25/c03038cf-42d9-4b76-bb2f-9a6a5a8e719c/","content":"\n### 1、概述\n旨在积累一些问题的解决方案和思路，下次出现快速解决。少走弯路；提高效率。\n<!-- more -->\n### 2、微服务分机部署\n* 分机部署，如果是不同服务商的服务器，加载速度会变得很慢。可以使用CDN加速。即使是同一个运营商不同地域的，也会这样。\n* 记得开放端口和设置安全组规则（比如阿里云的），想当于设置白名单。\n* 微服务的占用性能比较大，如果在出现线程被自动kill的现象，要即使查看是不是内存不足导致，从而出发了kill最大内存的进程。\n\n### 3、整理自己的资料\n* 学会使用标签。\n* Notes是个好东西。\n* 之后每份重要的文件都要放上标签，便于查找。\n* 自己的资料库要形成索引。","tags":["技术"]},{"title":"EHR系统员工生日提醒（跨年查询）总结与笔记","url":"/2018/12/19/81e30f7a-a061-4cfb-aace-39fdf4860955/","content":"\n### 1、概要\n生日跨年提醒，在EHR系统中，用户选择某个时间段都那些人过生日，并发送邮件提醒；本文只描述如何去跨年查询。\n前端使用Vue.js/Element UI;\n后端使用Java/Spring Cloud/微服务架构模式\n本文修改于：2019年03月15日\n<!-- more -->\n\n### 2、思路\n1、前端获取的是时间段，用户输入的时间段发送给后端的时候，判断一下用户选择的时间；\n2、如果没有跨年，则调用函数a；\n3、如果跨年了，则调用函数b；\n### 3、实现过程\n#### 3.1、前端实现\n###### 3.1.1、界面效果\n\n![](../../../../../../images/2.png)\n\nElement UI时间组件：[点击这里](http://element-cn.eleme.io/#/zh-CN/component/date-picker)\n\n###### 3.1.2、前端代码\n```javascript\n      handleFilter() {\n        this.searchkpi();\n        this.listQuery.startDataBirthdayPoint = this.employeeBirthday[0].toString().substring(5, 10).replace('-', '')\n        this.listQuery.endDataBirthdayPoint = this.employeeBirthday[1].toString().substring(5, 10).replace('-', '')\n        if (this.employeeBirthday[0].toString().substring(0, 4) === this.employeeBirthday[1].toString().substring(0, 4)) {\n          this.getListNotAcrossYear();\n        } else {\n          this.getList();\n        }\n      }\n      // 代码片段1\n```\n第5行代码，判断了年份是否相同，如果年份是否相同，如果相同，则调用<b>this.getListNotAcrossYear</b>, 调用接口：<b>pageBirthdayNotAcrossYear()</b>, 参数为this.listQuery;\n```javascript\n      getListNotAcrossYear() {\n        this.loading = true;\n        pageBirthdayNotAcrossYear(this.listQuery)\n          .then(response => {\n            this.list = response.data.rows;\n            this.total = response.data.total;\n            this.loading = false;\n          }).catch(error => {\n            console.error(error)\n            this.loading = false\n          })\n      }\n       // 代码片段2\n```\n<b>this.listQuery</b>包含两个字段：\n```javascript\n          startDataBirthdayPoint: undefined,\n          endDataBirthdayPoint: undefined\n           // 代码片段3\n```\n这两个字段，作为listQuery的一部分，作为参数，通过前端的API请求接口，传递给后端:\n```javascript\n// 生日提醒\nexport function pageBirthdayNotAcrossYear(query) {\n  return request({\n    url: '/api/admin/mEmployeeInfo/pageBirthdayNotAcrossYear',\n    method: 'get',\n    params: query\n  })\n}\n // 代码片段4\n```\n从接口中可以看到：传入的参数是<b>query</b>,使用GET的方式传递给后端。\n相同的，在<b>代码片段1</b>中的第8行，则是请求跨年的，在前端的代码部分都是一样的，这里我是在前端就对其进行了分开，后来想了想，如果日期传递到后端，在后端进行处理，代码量要少很多。\n#### 3.2、后端实现\n先看下不跨年的，包含Java代码和MyBatis的.xml映射文件；\n###### 3.2.1、Java代码\n首先是Controller类：\n```java\n@RequestMapping(\"mEmployeeInfo\")\n// 代码片段5\n```\n应和了在代码片段4中的请求接口；\n这是Java代码Controller的函数：\n```java\n    @RequestMapping(value = \"/pageBirthdayNotAcrossYear\", method = RequestMethod.GET)\n    @ResponseBody\n    public TableResultResponse<MEmployeeInfo> selectBirthdayNotAcrossYear(@RequestParam Map<String, Object> params) {\n        PageQuery query = new PageQuery(params, params);\n        return mEmployeeInfoBiz.selectBirthdayNotAcrossYear(query);\n    }\n    //代码片段6\n```\n\n函数的类型是<b>TableResultResponse</b>,返回的是一个结果集；\n\n这是Java代码Service类的函数：\n```java\n    // 生日到期提醒(不跨年)\n    public TableResultResponse<MEmployeeInfo> selectBirthdayNotAcrossYear(PageQuery<MEmployeeInfo> query) {\n        PageExample example = new PageExample(MEmployeeInfo.class, query.getData());\n        Page<MEmployeeInfo> result = PageHelper.startPage(query.getPage(), query.getLimit());\n        List<MEmployeeInfo> list = mEmployeeInfoMapper.selectBirthdayNotAcrossYear(example);\n        if (list.size() > 0){\n            try {\n                mergeCore.mergeResult(MEmployeeInfo.class, list);\n            } catch (Exception e) {\n                e.printStackTrace();\n            }\n        }\n        return  new TableResultResponse<MEmployeeInfo>(result.getTotal(), list);\n    }\n    // 代码片段7\n```\nDAO类\n```java\nList<MEmployeeInfo> selectBirthdayNotAcrossYear(Object example);\n// 代码片段8\n```\n#### 3.2.2、MyBatis的映射文件\n接下来是MyBatis的映射文件代码\n```java\n    <select id=\"selectBirthdayNotAcrossYear\" parameterType=\"com.savor.security.common.entity.PageExample\" resultMap=\"BaseResultMap\">\n        SELECT\n        t.*, y.position_name\n        FROM\n        m_employee_info t\n        LEFT JOIN m_position y ON t.position_id = y.position_id\n        WHERE\n        t.employee_id IS NOT NULL\n        <if test=\"data.startDataBirthdayPoint != null and data.startDataBirthdayPoint != '' and data.endDataBirthdayPoint != null and data.endDataBirthdayPoint !=''\">\n            AND DATE_FORMAT(t.employee_birthday , '%m%d') BETWEEN #{data.startDataBirthdayPoint} AND #{data.endDataBirthdayPoint}\n        </if>\n    </select>\n    -- 代码片段9\n```\n\n跨年的请求处理，跟上面的代码没有区别，不同之处体现在最后的映射文件这里：\n```java\n    <select id=\"selectByBirthday\" parameterType=\"com.savor.security.common.entity.PageExample\" resultMap=\"BaseResultMap\">\n        select a.* from (\n        SELECT\n        t.*, y.position_name\n        FROM\n        m_employee_info t\n        LEFT JOIN m_position y ON t.position_id = y.position_id\n        WHERE\n        t.employee_id IS NOT NULL\n        <if test=\"data.startDataBirthdayPoint != null and data.startDataBirthdayPoint != '' and data.endDataBirthdayPoint != null and data.endDataBirthdayPoint !=''\">\n            AND DATE_FORMAT(t.employee_birthday , '%m%d') BETWEEN #{data.startDataBirthdayPoint} AND #{data.endDataBirthdayPoint}\n        </if>\n        UNION ALL\n        SELECT\n        t.*, y.position_name\n        FROM\n        m_employee_info t\n        LEFT JOIN m_position y ON t.position_id = y.position_id\n        WHERE\n        t.employee_id IS NOT NULL\n        <if test=\"data.startDataBirthdayPoint != null and data.startDataBirthdayPoint != '' and data.endDataBirthdayPoint != null and data.endDataBirthdayPoint !=''\">\n            AND DATE_FORMAT(t.employee_birthday , '%m%d') BETWEEN #{data.startDataBirthdayPoint} AND '1231'\n        </if>\n        UNION ALL\n        SELECT\n        t.*, y.position_name\n        FROM\n        m_employee_info t\n        LEFT JOIN m_position y ON t.position_id = y.position_id\n        WHERE\n        t.employee_id IS NOT NULL\n        <if test=\"data.startDataBirthdayPoint != null and data.startDataBirthdayPoint != '' and data.endDataBirthdayPoint != null and data.endDataBirthdayPoint !=''\">\n            AND DATE_FORMAT(t.employee_birthday , '%m%d') BETWEEN '0101' AND #{data.endDataBirthdayPoint}\n        </if>\n        ) as a\n    </select>\n```\n其中也分成了三段，第一段就是正常的输入日期，但是不会去调用了。\n后两段是：\n如果跨年的时候，比如选择这个时间段2018-12-05~2019-03-15进行查询，那么数据流到映射文件这里的时候，会被分成两段（因为生日是考虑月日的，不考虑年）：就是12-05~12-31一部分人；01-01~03-15第二部分人；最后加起来，就是要的人数；\n\n","tags":["Spring Cloud"]},{"title":"前端开发Vue.js问题整理与总结","url":"/2018/12/18/c03d5240-9bbc-4ce5-83c5-32bace9f8c2b/","content":"## 背景\n* 所在的项目上前端开发使用技术栈为Vue.js。\n* 本文的主要内容开发时候遇到的问题、解决的方案、常用的解决方法。\n* 思路。\n<!-- more -->\n## 测试代码高亮\n\n```javascript\n// 简单语法\nVue.component('props-demo-simple', {\n  props: ['size', 'myMessage']\n})\n``\n// 对象语法，提供校验\nVue.component('props-demo-advanced', {\n  props: {\n    // 检测类型\n    height: Number,\n    // 检测类型 + 其他验证\n    age: {\n      type: Number,\n      default: 0,\n      required: true,\n      validator: function (value) {\n        return value >= 0\n      }\n    }\n  }\n})\n```\n## 组件弹窗\n* 原因:主要原因是没理解弹窗的原理。写的时候没有按照步骤和关键点进行。\n\n## JavaScript数据结构\n* 对JavaScript数组的理解不熟悉。\n* 对返回的数据做不出明确的判断来。\n\n## 组件通信\n\n## 数据绑定\n\n\n","tags":["Vue.js"]},{"title":"微服务模块的理解","url":"/2018/12/17/ff763a72-1a9f-4896-be36-d6dddd3b7d11/","content":"### 1、项目微服务结构整体理解\n### 2、背景\n* 各司其职、集中管理、存在单向或多向调用关系；\n<!-- more -->\n\n### 3、意义\n* 提高迭代效率；\n* 降低开发黏连性；\n* 节约成本\n\n### 4、实现\n### 5、结构\n#### 5.1、项目启动的入口CenterBootstrap\n* 各项服务的注册、管理中心。\n* 微服务请求转发。\n#### 5.2、鉴权（权限）服务AuthBootstrap\n* JWT处理权限\n#### 5.3、字典服务DictBootstrap\n* 管理项目的数据字典。\n#### 5.4、后台管理服务AdminBootstrap\n* 连接数据库、处理数据。\n#### 5.5、网关服务GateBootstrap\n* 接收前端的请求并转发至其他服务。","tags":["微服务"]},{"title":"iOS开发基础之学习Objective-C笔记一","url":"/2018/12/11/f72e9838-7a1a-4f0f-ab4d-0146231490a1/","content":"### 1、前言\n<!-- more -->\n\n### 2、背景\n* 学习一门新的手艺。\n* 做个小的Demo，解决自己对于现有的某些工具不能满足自己需求的痛点。\n* Apple Store一些应用需要花钱。\n\n### 3、计划\n* 第一、先看书。重点关注一下指针这块。\n* 第二、照着实例写一写UI。画一画界面。\n* 第三、写一写后台的逻辑。实现一些小的功能。\n* 第四、整合记录一下笔记和发布一下源码。提升一下。\n* 第五、规划一个tools的功能，写计划书，准备开发。","tags":["Objective-C"]}]